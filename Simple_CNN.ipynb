{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joint-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "typical-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd #Basic library for all of our dataset operations\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "import pmdarima as pm\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "import gluonts\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# # import shap\n",
    "# # warnings.filterwarnings(\"ignore\") #We will use deprecated models of statmodels which throw a lot of warnings to use more modern ones\n",
    "\n",
    "from utils.metrics import evaluate\n",
    "from utils.plots import bar_metrics\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "from fbprophet import Prophet\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from math import sqrt\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from itertools import islice\n",
    "from pylab import rcParams\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "#Extra settings\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.style.use('bmh')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spanish-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highest</th>\n",
       "      <th>high</th>\n",
       "      <th>mid</th>\n",
       "      <th>low</th>\n",
       "      <th>ave</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-12-01</th>\n",
       "      <td>195</td>\n",
       "      <td>131</td>\n",
       "      <td>77</td>\n",
       "      <td>59</td>\n",
       "      <td>84.0</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-02</th>\n",
       "      <td>165</td>\n",
       "      <td>144</td>\n",
       "      <td>111</td>\n",
       "      <td>93</td>\n",
       "      <td>113.0</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-03</th>\n",
       "      <td>204</td>\n",
       "      <td>160</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>94.0</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-04</th>\n",
       "      <td>189</td>\n",
       "      <td>161</td>\n",
       "      <td>114</td>\n",
       "      <td>80</td>\n",
       "      <td>116.0</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-06</th>\n",
       "      <td>181</td>\n",
       "      <td>157</td>\n",
       "      <td>90</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-26</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>137</td>\n",
       "      <td>67</td>\n",
       "      <td>134.9</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-27</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>112</td>\n",
       "      <td>52</td>\n",
       "      <td>111.8</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-28</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>170</td>\n",
       "      <td>152</td>\n",
       "      <td>166.7</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>172</td>\n",
       "      <td>171</td>\n",
       "      <td>119</td>\n",
       "      <td>82</td>\n",
       "      <td>122.4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>199</td>\n",
       "      <td>190</td>\n",
       "      <td>200.5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6644 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            highest  high  mid  low    ave  volume\n",
       "date                                              \n",
       "1998-12-01      195   131   77   59   84.0     327\n",
       "1998-12-02      165   144  111   93  113.0     456\n",
       "1998-12-03      204   160   82   65   94.0     432\n",
       "1998-12-04      189   161  114   80  116.0     583\n",
       "1998-12-06      181   157   90   55   96.0    1052\n",
       "...             ...   ...  ...  ...    ...     ...\n",
       "2020-11-26      201   201  137   67  134.9      97\n",
       "2020-11-27      170   170  112   52  111.8     176\n",
       "2020-11-28      171   171  170  152  166.7      81\n",
       "2020-11-30      172   171  119   82  122.4      88\n",
       "2020-12-01      212   212  199  190  200.5      23\n",
       "\n",
       "[6644 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_col = ['date', 'highest', 'high', 'mid', 'low', 'ave', 'ave_dif', 'volume', 'vol_dif', 'resid']\n",
    "raw = pd.read_csv('FS443.csv',names=raw_col).drop(['ave_dif', 'vol_dif', 'resid'], axis=1)\n",
    "for i in range(raw.shape[0]):\n",
    "    raw.volume[i] = int(raw.volume[i].split(' ')[0].replace(',', ''))\n",
    "raw.volume = raw.volume.values.astype('int')\n",
    "\n",
    "ts_date = raw.copy().date\n",
    "for itr in range(raw.shape[0]):\n",
    "    date = raw.date[itr].split('/')\n",
    "    date[0] = str(int(date[0]) + 1911)\n",
    "    result = '/'.join(date)\n",
    "    ts_date[itr] =  np.datetime64(datetime.strptime(result, \"%Y/%m/%d\").date())\n",
    "    \n",
    "\n",
    "\n",
    "ts = raw.copy()\n",
    "ts.date = ts_date\n",
    "ts['date'] = pd.to_datetime(ts['date'], errors='coerce')\n",
    "ts = ts.set_index('date')\n",
    "ts.index = pd.to_datetime(ts.index)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts.copy()\n",
    "df['date'] = ts.index\n",
    "df['hour'] = df['date'].dt.hour.astype(int)\n",
    "df['dayofweek'] = df['date'].dt.dayofweek.astype(int)\n",
    "df['quarter'] = df['date'].dt.quarter.astype(int)\n",
    "df['month'] = df['date'].dt.month.astype(int)\n",
    "df['year'] = df['date'].dt.year.astype(int)\n",
    "df['dayofyear'] = df['date'].dt.dayofyear.astype(int)\n",
    "df['dayofmonth'] = df['date'].dt.day.astype(int)\n",
    "df['weekofyear'] = df['date'].dt.weekofyear.astype(int)\n",
    "df = df.drop(['hour','date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "romance-liberty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highest</th>\n",
       "      <th>high</th>\n",
       "      <th>mid</th>\n",
       "      <th>low</th>\n",
       "      <th>ave</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-12-01</th>\n",
       "      <td>195</td>\n",
       "      <td>131</td>\n",
       "      <td>77</td>\n",
       "      <td>59</td>\n",
       "      <td>84.0</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-02</th>\n",
       "      <td>165</td>\n",
       "      <td>144</td>\n",
       "      <td>111</td>\n",
       "      <td>93</td>\n",
       "      <td>113.0</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-03</th>\n",
       "      <td>204</td>\n",
       "      <td>160</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>94.0</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-04</th>\n",
       "      <td>189</td>\n",
       "      <td>161</td>\n",
       "      <td>114</td>\n",
       "      <td>80</td>\n",
       "      <td>116.0</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-06</th>\n",
       "      <td>181</td>\n",
       "      <td>157</td>\n",
       "      <td>90</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-26</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>137</td>\n",
       "      <td>67</td>\n",
       "      <td>134.9</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-27</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>112</td>\n",
       "      <td>52</td>\n",
       "      <td>111.8</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-28</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>170</td>\n",
       "      <td>152</td>\n",
       "      <td>166.7</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>172</td>\n",
       "      <td>171</td>\n",
       "      <td>119</td>\n",
       "      <td>82</td>\n",
       "      <td>122.4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>199</td>\n",
       "      <td>190</td>\n",
       "      <td>200.5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6644 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            highest  high  mid  low    ave  volume\n",
       "date                                              \n",
       "1998-12-01      195   131   77   59   84.0     327\n",
       "1998-12-02      165   144  111   93  113.0     456\n",
       "1998-12-03      204   160   82   65   94.0     432\n",
       "1998-12-04      189   161  114   80  116.0     583\n",
       "1998-12-06      181   157   90   55   96.0    1052\n",
       "...             ...   ...  ...  ...    ...     ...\n",
       "2020-11-26      201   201  137   67  134.9      97\n",
       "2020-11-27      170   170  112   52  111.8     176\n",
       "2020-11-28      171   171  170  152  166.7      81\n",
       "2020-11-30      172   171  119   82  122.4      88\n",
       "2020-12-01      212   212  199  190  200.5      23\n",
       "\n",
       "[6644 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "color-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ts.ave[:-14].values.reshape(-1, 1)\n",
    "test_set = ts.ave[-14:].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "magnetic-literacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6630, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "champion-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6621, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(training_set_scaled)\n",
    "dataset = dataset.window(10, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(10))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "for x,y in dataset:\n",
    "    train_x.append(x.numpy())\n",
    "    train_y.append(y.numpy())\n",
    "\n",
    "X_train, y_train = np.array(train_x), np.array(train_y)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "rotary-johns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = ts[-23:].ave.values.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(inputs) #20 = 14 + (5-1) -> windows size -1\n",
    "dataset = dataset.window(10, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(10))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "# dataset = dataset.shuffle(buffer_size=10)\n",
    "test_x = []\n",
    "test_y = []\n",
    "for x,y in dataset:\n",
    "    test_x.append(x.numpy())\n",
    "    test_y.append(y.numpy())\n",
    "\n",
    "X_test, y_test = np.array(test_x), np.array(test_y)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-treatment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "homeless-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "separate-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "trans = sc.fit_transform(ts.ave.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "noted-vatican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6644,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "domestic-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_dataset_nodate(ts,df,window_size):\n",
    "    w = window_size\n",
    "    \n",
    "    sc = MinMaxScaler(feature_range = (0, 1))\n",
    "    trans = sc.fit_transform(ts.ave.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(trans)\n",
    "    dataset = dataset.window(w, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(w))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "    target = []\n",
    "    x_ser = []\n",
    "    for x,y in dataset:\n",
    "        x_ser.append(x.numpy().ravel())\n",
    "        target.append(y.numpy())\n",
    "        \n",
    "\n",
    "    return np.array(x_ser), np.array(target),sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "printable-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def window_dataset_nodate(ts,df,window_size):\n",
    "#     w = window_size\n",
    "    \n",
    "#     sc = MinMaxScaler(feature_range = (0, 1))\n",
    "#     training_set_scaled = sc.fit_transform(df.values)\n",
    "#     sc.transform(inputs)\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "#     dataset = dataset.window(w, shift=1, drop_remainder=True)\n",
    "#     dataset = dataset.flat_map(lambda window: window.batch(w))\n",
    "#     dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "#     target = []\n",
    "#     x_date = []\n",
    "#     for x,y in dataset:\n",
    "\n",
    "\n",
    "#         target.append(y.numpy()[0,4])\n",
    "\n",
    "\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(ts.ave.values)\n",
    "#     dataset = dataset.window(w, shift=1, drop_remainder=True)\n",
    "#     dataset = dataset.flat_map(lambda window: window.batch(w))\n",
    "#     dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "#     x_ser = []\n",
    "#     for x,y in dataset:\n",
    "#         x_ser.append(x.numpy().ravel())\n",
    "        \n",
    "\n",
    "#     return np.array(x_ser), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "minus-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df, y_df, sc = window_dataset_nodate(ts,df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "victorian-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_df, y_df, test_size=0.002107164358819988, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# sc = MinMaxScaler(feature_range = (0, 1))\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "juvenile-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "207/207 [==============================] - 14s 17ms/step - loss: 0.0311\n",
      "Epoch 2/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0109\n",
      "Epoch 3/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0098\n",
      "Epoch 4/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0080\n",
      "Epoch 5/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0067\n",
      "Epoch 6/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0057\n",
      "Epoch 7/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0055\n",
      "Epoch 8/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0053\n",
      "Epoch 9/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0053\n",
      "Epoch 10/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0050\n",
      "Epoch 11/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0050\n",
      "Epoch 12/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0048\n",
      "Epoch 13/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0047\n",
      "Epoch 14/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0049\n",
      "Epoch 15/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0047\n",
      "Epoch 16/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0044\n",
      "Epoch 17/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0046\n",
      "Epoch 18/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0047\n",
      "Epoch 19/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0044\n",
      "Epoch 20/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 21/100\n",
      "207/207 [==============================] - 3s 16ms/step - loss: 0.0044\n",
      "Epoch 22/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0046\n",
      "Epoch 23/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 24/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 25/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0044\n",
      "Epoch 26/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 27/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 28/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0044\n",
      "Epoch 29/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0046\n",
      "Epoch 30/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0043\n",
      "Epoch 31/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 32/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0043\n",
      "Epoch 33/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 34/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 35/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0039\n",
      "Epoch 36/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0040\n",
      "Epoch 37/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 38/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 39/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 40/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 41/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0043\n",
      "Epoch 42/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0041\n",
      "Epoch 43/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0042\n",
      "Epoch 44/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0042\n",
      "Epoch 45/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 46/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0038\n",
      "Epoch 47/100\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 0.0039\n",
      "Epoch 48/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 49/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0040\n",
      "Epoch 50/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0040\n",
      "Epoch 51/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 52/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0044\n",
      "Epoch 53/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0046\n",
      "Epoch 54/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0041\n",
      "Epoch 55/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0039\n",
      "Epoch 56/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0041\n",
      "Epoch 57/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0043\n",
      "Epoch 58/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 59/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0039: 1s - loss: 0.00 - ETA:\n",
      "Epoch 60/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0038\n",
      "Epoch 61/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 62/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 63/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0045\n",
      "Epoch 64/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 65/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0043\n",
      "Epoch 66/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 67/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0038\n",
      "Epoch 68/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 69/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 70/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 71/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 72/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 73/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 74/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0037\n",
      "Epoch 75/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 76/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 77/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 78/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 79/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 80/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 81/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 82/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 83/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 84/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 85/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 86/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0043\n",
      "Epoch 87/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 88/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0046\n",
      "Epoch 89/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 90/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 91/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 92/100\n",
      "207/207 [==============================] - 5s 23ms/step - loss: 0.0040\n",
      "Epoch 93/100\n",
      "207/207 [==============================] - 5s 24ms/step - loss: 0.0041\n",
      "Epoch 94/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 95/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 96/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0037\n",
      "Epoch 97/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 98/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 99/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 100/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c7fea0fc8>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "boring-documentary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "declared-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "unexpected-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149.],\n",
       "       [145.],\n",
       "       [195.],\n",
       "       [104.],\n",
       "       [136.],\n",
       "       [171.],\n",
       "       [174.],\n",
       "       [158.],\n",
       "       [114.],\n",
       "       [ 96.],\n",
       "       [210.],\n",
       "       [152.],\n",
       "       [151.],\n",
       "       [ 75.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "sharp-chuck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158.82239],\n",
       "       [150.1323 ],\n",
       "       [198.00038],\n",
       "       [118.53249],\n",
       "       [153.83786],\n",
       "       [198.14963],\n",
       "       [158.80101],\n",
       "       [174.96297],\n",
       "       [128.14429],\n",
       "       [ 98.81711],\n",
       "       [211.452  ],\n",
       "       [126.92655],\n",
       "       [149.32808],\n",
       "       [ 87.73979]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "packed-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Root Mean Squared Error of our forecasts is 14.44\n"
     ]
    }
   ],
   "source": [
    "mse0 = ((sc.inverse_transform(y_test) - predicted_stock_price) ** 2).mean()\n",
    "\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-sherman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "direct-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "207/207 [==============================] - 22s 21ms/step - loss: 0.0294\n",
      "Epoch 2/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0115\n",
      "Epoch 3/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0105\n",
      "Epoch 4/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0082\n",
      "Epoch 5/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0069\n",
      "Epoch 6/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0060\n",
      "Epoch 7/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0054\n",
      "Epoch 8/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0055\n",
      "Epoch 9/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0049\n",
      "Epoch 10/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0049: 0s \n",
      "Epoch 11/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0050\n",
      "Epoch 12/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0044\n",
      "Epoch 13/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0044\n",
      "Epoch 14/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0045\n",
      "Epoch 15/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0045\n",
      "Epoch 16/100\n",
      "207/207 [==============================] - 4s 22ms/step - loss: 0.0046\n",
      "Epoch 17/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0047\n",
      "Epoch 18/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0045\n",
      "Epoch 19/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0045\n",
      "Epoch 20/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0045\n",
      "Epoch 21/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0046\n",
      "Epoch 22/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 23/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 24/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 25/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0045\n",
      "Epoch 26/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 27/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 28/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 29/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 30/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 31/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 32/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0045\n",
      "Epoch 33/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 34/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0043\n",
      "Epoch 35/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 36/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 37/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 38/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042: 0s\n",
      "Epoch 39/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 40/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 41/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0048\n",
      "Epoch 42/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0043\n",
      "Epoch 43/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 44/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0042\n",
      "Epoch 45/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 46/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 47/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0046\n",
      "Epoch 48/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 49/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0042\n",
      "Epoch 50/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 51/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0045\n",
      "Epoch 52/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 53/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0042\n",
      "Epoch 54/100\n",
      "207/207 [==============================] - 5s 22ms/step - loss: 0.0041: 0s -  - ETA: 0s - loss: 0.00\n",
      "Epoch 55/100\n",
      "207/207 [==============================] - 4s 22ms/step - loss: 0.0041\n",
      "Epoch 56/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 57/100\n",
      "207/207 [==============================] - 5s 23ms/step - loss: 0.0038\n",
      "Epoch 58/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 59/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0039\n",
      "Epoch 60/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 61/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 62/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0039\n",
      "Epoch 63/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 64/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 65/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 66/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0041\n",
      "Epoch 67/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0037\n",
      "Epoch 68/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0042\n",
      "Epoch 69/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0041\n",
      "Epoch 70/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0039\n",
      "Epoch 71/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 72/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0038\n",
      "Epoch 73/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0042\n",
      "Epoch 74/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 75/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 76/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0038\n",
      "Epoch 77/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 78/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 79/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 80/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 81/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 82/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 83/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 84/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 85/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 86/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0039\n",
      "Epoch 87/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 88/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0041\n",
      "Epoch 89/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0039\n",
      "Epoch 90/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 91/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0041: \n",
      "Epoch 92/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0040\n",
      "Epoch 93/100\n",
      "207/207 [==============================] - 5s 22ms/step - loss: 0.0037\n",
      "Epoch 94/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0042\n",
      "Epoch 95/100\n",
      "207/207 [==============================] - 5s 23ms/step - loss: 0.0037\n",
      "Epoch 96/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 97/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0041\n",
      "Epoch 99/100\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 0.0040\n",
      "Epoch 100/100\n",
      "207/207 [==============================] - 4s 18ms/step - loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c0cc25d48>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_df, y_df, test_size=0.002107164358819988, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "# sc = MinMaxScaler(feature_range = (0, 1))\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "anonymous-geography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Root Mean Squared Error of our forecasts is 27.35\n"
     ]
    }
   ],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "mse1 = ((sc.inverse_transform(y_test) - predicted_stock_price) ** 2).mean()\n",
    "\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse1), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-tragedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dress-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "207/207 [==============================] - 19s 19ms/step - loss: 0.0305\n",
      "Epoch 2/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0117\n",
      "Epoch 3/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0097\n",
      "Epoch 4/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0077: \n",
      "Epoch 5/100\n",
      "207/207 [==============================] - 4s 22ms/step - loss: 0.0071\n",
      "Epoch 6/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0058: 0\n",
      "Epoch 7/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0052\n",
      "Epoch 8/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0053\n",
      "Epoch 9/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0048\n",
      "Epoch 10/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0052\n",
      "Epoch 11/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0047\n",
      "Epoch 12/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0050\n",
      "Epoch 13/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0047\n",
      "Epoch 14/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0047\n",
      "Epoch 15/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0046\n",
      "Epoch 16/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0046\n",
      "Epoch 17/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0048\n",
      "Epoch 18/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 19/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 20/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 21/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0044\n",
      "Epoch 22/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 23/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 24/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 25/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 26/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 27/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 28/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 29/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 30/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 31/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 32/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 33/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 34/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 35/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 36/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 37/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 38/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 39/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 40/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 41/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 42/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 43/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 44/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 45/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 46/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 47/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 48/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 49/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 50/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0046\n",
      "Epoch 51/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 52/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 53/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 54/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 55/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 56/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 57/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 58/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 59/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 60/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 61/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 62/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 63/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 64/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 65/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 66/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 67/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 68/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0041\n",
      "Epoch 69/100\n",
      "207/207 [==============================] - 5s 26ms/step - loss: 0.0043\n",
      "Epoch 70/100\n",
      "207/207 [==============================] - 5s 22ms/step - loss: 0.0040: 0s - loss\n",
      "Epoch 71/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0039\n",
      "Epoch 72/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 73/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0041\n",
      "Epoch 74/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 75/100\n",
      "207/207 [==============================] - 4s 21ms/step - loss: 0.0037\n",
      "Epoch 76/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 77/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0042\n",
      "Epoch 78/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 79/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 80/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 81/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0037\n",
      "Epoch 82/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 83/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 84/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0039\n",
      "Epoch 85/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040: 0s - \n",
      "Epoch 86/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0038\n",
      "Epoch 87/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 88/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 89/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0042\n",
      "Epoch 90/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 91/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0041\n",
      "Epoch 92/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n",
      "Epoch 93/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0044\n",
      "Epoch 94/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0041\n",
      "Epoch 95/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 96/100\n",
      "207/207 [==============================] - 4s 20ms/step - loss: 0.0040\n",
      "Epoch 97/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0038\n",
      "Epoch 98/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0043\n",
      "Epoch 99/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0042\n",
      "Epoch 100/100\n",
      "207/207 [==============================] - 4s 19ms/step - loss: 0.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c815724c8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_df, y_df, test_size=0.002107164358819988, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "mse2 = ((sc.inverse_transform(y_test) - predicted_stock_price) ** 2).mean()\n",
    "\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse2), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-positive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_df, y_df, test_size=0.002107164358819988, random_state=3)\n",
    "\n",
    "\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "mse3 = ((sc.inverse_transform(y_test) - predicted_stock_price) ** 2).mean()\n",
    "\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse3), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-treasure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_df, y_df, test_size=0.002107164358819988, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "mse4 = ((sc.inverse_transform(y_test) - predicted_stock_price) ** 2).mean()\n",
    "\n",
    "print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse4), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
